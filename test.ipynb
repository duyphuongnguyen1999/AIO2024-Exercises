{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden layer output (h): tensor([[3.2000, 0.0000]])\n",
      "Output layer logits (z): tensor([[0.9600, 0.6400, 0.3200]])\n",
      "Softmax output (ŷ): tensor([[0.4438, 0.3222, 0.2340]])\n",
      "Loss value in first forward propagation:  tensor(0.8125)\n"
     ]
    }
   ],
   "source": [
    "# Input values\n",
    "X = torch.tensor([[3.0, 2.0]], dtype=torch.float32)\n",
    "\n",
    "# Ground-truth\n",
    "y_true = torch.tensor([0]) \n",
    "\n",
    "# Weights and biases for hidden layer\n",
    "Wh = torch.tensor([[0.8, -1.0],\n",
    "                   [0.4, -0.6]], dtype=torch.float32)  # 2x2\n",
    "bh = torch.tensor([0.0, 0.0], dtype=torch.float32)  # Bias for hidden layer\n",
    "\n",
    "# Weights and biases for output layer\n",
    "Wz = torch.tensor([[0.3, 0.2, 0.1],\n",
    "                   [-0.4, -1.0, 0.2]], dtype=torch.float32)  # 2x3\n",
    "bz = torch.tensor([0.0, 0.0, 0.0], dtype=torch.float32)  # Bias for output layer\n",
    "\n",
    "# Hidden layer computation: h = ReLU(X @ Wh + bh)\n",
    "h = F.relu(torch.matmul(X, Wh) + bh)\n",
    "\n",
    "# Output layer computation: z = h @ Wz + bz\n",
    "z = torch.matmul(h, Wz) + bz\n",
    "\n",
    "# Softmax to calculate probabilities\n",
    "y_hat = F.softmax(z, dim=1)\n",
    "\n",
    "# Loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "loss = loss_fn(z, y_true)\n",
    "\n",
    "# Print results\n",
    "print(\"Hidden layer output (h):\", h)\n",
    "print(\"Output layer logits (z):\", z)\n",
    "print(\"Softmax output (ŷ):\", y_hat)\n",
    "print(\"Loss value in first forward propagation: \", loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax output (ŷ): tensor([[0.3333, 0.3333, 0.3333]], grad_fn=<SoftmaxBackward0>)\n",
      "Loss: 1.0986123085021973\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Initialize a Hidden layer\n",
    "hidden_layer = nn.Linear(2, 2)\n",
    "hidden_layer.weight.data = torch.tensor([[0.8, 0.4],\n",
    "                                         [-1.0, -0.6]], dtype=torch.float32)\n",
    "hidden_layer.bias.data = torch.tensor([0.0, 0.0])\n",
    "\n",
    "# Output layer\n",
    "output_layer = nn.Linear(2, 3)\n",
    "# Initialize Weight and Bias matrices\n",
    "output_layer.weight.data = torch.tensor([[1, 1],\n",
    "                                         [1, 1],\n",
    "                                         [1, 1]], dtype=torch.float32)\n",
    "output_layer.bias.data = torch.tensor([0.0, 0.0, 0.0], dtype=torch.float32)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Define the model using nn.Linear and nn.ReLU\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.hidden_layer = hidden_layer\n",
    "        self.output_layer = output_layer\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Pass input through hidden layer and apply ReLU activation\n",
    "        h = F.relu(self.hidden_layer(x))\n",
    "        # Pass hidden layer output through output layer\n",
    "        z = self.output_layer(h)\n",
    "        # Apply softmax to get probabilities\n",
    "        y_hat = F.softmax(z, dim=1)\n",
    "        return y_hat\n",
    "\n",
    "# Initialize the model\n",
    "model = SimpleModel()\n",
    "\n",
    "# Loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Example input\n",
    "X = torch.tensor([[3.0, 2.0]], dtype=torch.float32)  # Batch of size 1\n",
    "\n",
    "# Forward pass\n",
    "output = model(X)\n",
    "\n",
    "# Print the softmax output (probabilities)\n",
    "print(\"Softmax output (ŷ):\", output)\n",
    "\n",
    "# Assume the true class label for the sample is class 1\n",
    "y_true = torch.tensor([1])  # Target label\n",
    "\n",
    "# Compute loss (CrossEntropyLoss expects raw logits, so no softmax here)\n",
    "z = model.output_layer(F.relu(model.hidden_layer(X)))  # Get logits\n",
    "loss = loss_fn(z, y_true)\n",
    "\n",
    "# Print loss\n",
    "print(\"Loss:\", loss.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIO2024-Exercises",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
