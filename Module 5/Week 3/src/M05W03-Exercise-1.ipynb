{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M05W03 - Multilayer Perceptrons (MLPs) and Activation Functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random state set up\n",
    "random_state = 59\n",
    "np.random.seed(random_state)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(random_state)\n",
    "else:\n",
    "    torch.manual_seed(random_state)\n",
    "    \n",
    "# Set up Pytorch computational device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project 1: Auto MPG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "val_size = 0.2\n",
    "test_size = 0.125\n",
    "is_shuffle = True\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement CustomDataset class to store input data\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.X = x\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "dataset_dir = 'data'\n",
    "dataset_filename = 'Auto_MPG_data.csv'\n",
    "dataset_path = os.path.join('..', dataset_dir, dataset_filename)\n",
    "dataset = pd.read_csv(dataset_path)\n",
    "\n",
    "# Data Preprocessing\n",
    "X = dataset.drop(columns='MPG').to_numpy()\n",
    "y = dataset['MPG'].to_numpy()\n",
    "\n",
    "# Train - val - test split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y,\n",
    "    test_size=val_size,\n",
    "    random_state=random_state,\n",
    "    shuffle=is_shuffle\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_train, y_train,\n",
    "    test_size=test_size,\n",
    "    random_state=random_state,\n",
    "    shuffle=is_shuffle\n",
    ")\n",
    "\n",
    "# Standardization\n",
    "normalizer = StandardScaler()\n",
    "X_train = normalizer.fit_transform(X_train)\n",
    "X_val = normalizer.transform(X_val)\n",
    "X_test = normalizer.transform(X_test)\n",
    "\n",
    "# Convert to Pytorch tensor\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "# Initialize CustomDataset obj\n",
    "train_dataset = CustomDataset(X_train, y_train)\n",
    "val_dataset = CustomDataset(X_val, y_val)\n",
    "test_dataset = CustomDataset(X_test, y_test)\n",
    "\n",
    "# Initialize DataLoader\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          num_workers=0)\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True,\n",
    "                        num_workers=0)\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                        batch_size=batch_size,\n",
    "                        shuffle=True,\n",
    "                        num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement MLP structure\n",
    "class MLP (nn.Module):\n",
    "    def __init__ (self, input_dims, hidden_dims, output_dims):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_dims, hidden_dims)\n",
    "        self.linear2 = nn.Linear(hidden_dims, hidden_dims)\n",
    "        self.output = nn.Linear(hidden_dims, output_dims)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.linear1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = F.relu(x)\n",
    "        out = self.output(x)\n",
    "        return out.squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "input_dims = X_train.shape[1]\n",
    "output_dims = 1\n",
    "hidden_dims = 64\n",
    "\n",
    "\n",
    "# Model intialization\n",
    "model = MLP(input_dims=input_dims,\n",
    "            hidden_dims=hidden_dims,\n",
    "            output_dims=output_dims).to(device)\n",
    "\n",
    "# Loss Function Selection\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Optimizer Selection\n",
    "momentum = 0.0\n",
    "weight_decay = 0.0\n",
    "lr = 1e-2\n",
    "optimizer = torch.optim.SGD(model.parameters(), \n",
    "                            momentum=momentum,\n",
    "                            weight_decay=weight_decay,\n",
    "                            lr=lr)\n",
    "\n",
    "# Metric Selection\n",
    "def r_squared(y_true, y_pred):\n",
    "    y_true = torch.Tensor(y_true).to(device)\n",
    "    y_pred = torch.Tensor(y_pred).to(device)\n",
    "    mean_true = torch.mean(y_true)\n",
    "    ss_tot = torch.sum((y_true - mean_true) ** 2)\n",
    "    ss_res = torch.sum((y_true - y_pred) ** 2)\n",
    "    r2 = 1 - (ss_res / ss_tot)\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EPOCH 1:\tTraining Loss: 264.166\tValidation loss: 556.237\n",
      "\n",
      "EPOCH 2:\tTraining Loss: 279.203\tValidation loss: 184.840\n",
      "\n",
      "EPOCH 3:\tTraining Loss: 112.318\tValidation loss: 215.240\n",
      "\n",
      "EPOCH 4:\tTraining Loss: 52.179\tValidation loss: 24.586\n",
      "\n",
      "EPOCH 5:\tTraining Loss: 39.055\tValidation loss: 6.259\n",
      "\n",
      "EPOCH 6:\tTraining Loss: 24.509\tValidation loss: 12.618\n",
      "\n",
      "EPOCH 7:\tTraining Loss: 22.156\tValidation loss: 131.026\n",
      "\n",
      "EPOCH 8:\tTraining Loss: 95.069\tValidation loss: 19.908\n",
      "\n",
      "EPOCH 9:\tTraining Loss: 18.043\tValidation loss: 17.442\n",
      "\n",
      "EPOCH 10:\tTraining Loss: 15.602\tValidation loss: 6.905\n",
      "\n",
      "EPOCH 11:\tTraining Loss: 10.203\tValidation loss: 9.593\n",
      "\n",
      "EPOCH 12:\tTraining Loss: 13.507\tValidation loss: 11.153\n",
      "\n",
      "EPOCH 13:\tTraining Loss: 14.607\tValidation loss: 10.934\n",
      "\n",
      "EPOCH 14:\tTraining Loss: 9.423\tValidation loss: 4.750\n",
      "\n",
      "EPOCH 15:\tTraining Loss: 21.439\tValidation loss: 7.417\n",
      "\n",
      "EPOCH 16:\tTraining Loss: 9.123\tValidation loss: 6.131\n",
      "\n",
      "EPOCH 17:\tTraining Loss: 13.770\tValidation loss: 5.305\n",
      "\n",
      "EPOCH 18:\tTraining Loss: 6.803\tValidation loss: 7.319\n",
      "\n",
      "EPOCH 19:\tTraining Loss: 6.981\tValidation loss: 9.259\n",
      "\n",
      "EPOCH 20:\tTraining Loss: 12.274\tValidation loss: 6.204\n",
      "\n",
      "EPOCH 21:\tTraining Loss: 8.413\tValidation loss: 5.943\n",
      "\n",
      "EPOCH 22:\tTraining Loss: 6.183\tValidation loss: 10.412\n",
      "\n",
      "EPOCH 23:\tTraining Loss: 9.318\tValidation loss: 5.470\n",
      "\n",
      "EPOCH 24:\tTraining Loss: 7.854\tValidation loss: 31.942\n",
      "\n",
      "EPOCH 25:\tTraining Loss: 16.725\tValidation loss: 4.670\n",
      "\n",
      "EPOCH 26:\tTraining Loss: 6.203\tValidation loss: 5.090\n",
      "\n",
      "EPOCH 27:\tTraining Loss: 6.679\tValidation loss: 7.507\n",
      "\n",
      "EPOCH 28:\tTraining Loss: 8.353\tValidation loss: 5.216\n",
      "\n",
      "EPOCH 29:\tTraining Loss: 11.826\tValidation loss: 4.720\n",
      "\n",
      "EPOCH 30:\tTraining Loss: 6.463\tValidation loss: 6.271\n",
      "\n",
      "EPOCH 31:\tTraining Loss: 5.800\tValidation loss: 5.386\n",
      "\n",
      "EPOCH 32:\tTraining Loss: 7.409\tValidation loss: 6.182\n",
      "\n",
      "EPOCH 33:\tTraining Loss: 6.685\tValidation loss: 38.745\n",
      "\n",
      "EPOCH 34:\tTraining Loss: 18.977\tValidation loss: 6.327\n",
      "\n",
      "EPOCH 35:\tTraining Loss: 8.148\tValidation loss: 7.438\n",
      "\n",
      "EPOCH 36:\tTraining Loss: 8.436\tValidation loss: 8.095\n",
      "\n",
      "EPOCH 37:\tTraining Loss: 6.486\tValidation loss: 6.041\n",
      "\n",
      "EPOCH 38:\tTraining Loss: 14.827\tValidation loss: 15.334\n",
      "\n",
      "EPOCH 39:\tTraining Loss: 12.759\tValidation loss: 9.063\n",
      "\n",
      "EPOCH 40:\tTraining Loss: 6.970\tValidation loss: 5.163\n",
      "\n",
      "EPOCH 41:\tTraining Loss: 6.539\tValidation loss: 4.212\n",
      "\n",
      "EPOCH 42:\tTraining Loss: 6.873\tValidation loss: 6.809\n",
      "\n",
      "EPOCH 43:\tTraining Loss: 6.079\tValidation loss: 6.430\n",
      "\n",
      "EPOCH 44:\tTraining Loss: 9.690\tValidation loss: 8.275\n",
      "\n",
      "EPOCH 45:\tTraining Loss: 8.354\tValidation loss: 6.484\n",
      "\n",
      "EPOCH 46:\tTraining Loss: 6.215\tValidation loss: 7.103\n",
      "\n",
      "EPOCH 47:\tTraining Loss: 5.878\tValidation loss: 5.489\n",
      "\n",
      "EPOCH 48:\tTraining Loss: 5.564\tValidation loss: 4.683\n",
      "\n",
      "EPOCH 49:\tTraining Loss: 5.794\tValidation loss: 4.981\n",
      "\n",
      "EPOCH 50:\tTraining Loss: 7.443\tValidation loss: 6.354\n",
      "\n",
      "EPOCH 51:\tTraining Loss: 6.634\tValidation loss: 6.859\n",
      "\n",
      "EPOCH 52:\tTraining Loss: 10.050\tValidation loss: 6.716\n",
      "\n",
      "EPOCH 53:\tTraining Loss: 7.222\tValidation loss: 8.035\n",
      "\n",
      "EPOCH 54:\tTraining Loss: 6.588\tValidation loss: 20.706\n",
      "\n",
      "EPOCH 55:\tTraining Loss: 12.078\tValidation loss: 32.232\n",
      "\n",
      "EPOCH 56:\tTraining Loss: 10.989\tValidation loss: 11.870\n",
      "\n",
      "EPOCH 57:\tTraining Loss: 8.349\tValidation loss: 6.950\n",
      "\n",
      "EPOCH 58:\tTraining Loss: 7.362\tValidation loss: 5.757\n",
      "\n",
      "EPOCH 59:\tTraining Loss: 5.396\tValidation loss: 4.476\n",
      "\n",
      "EPOCH 60:\tTraining Loss: 6.678\tValidation loss: 4.865\n",
      "\n",
      "EPOCH 61:\tTraining Loss: 5.331\tValidation loss: 6.793\n",
      "\n",
      "EPOCH 62:\tTraining Loss: 11.058\tValidation loss: 6.313\n",
      "\n",
      "EPOCH 63:\tTraining Loss: 6.939\tValidation loss: 5.453\n",
      "\n",
      "EPOCH 64:\tTraining Loss: 5.755\tValidation loss: 7.998\n",
      "\n",
      "EPOCH 65:\tTraining Loss: 5.062\tValidation loss: 6.143\n",
      "\n",
      "EPOCH 66:\tTraining Loss: 4.733\tValidation loss: 5.838\n",
      "\n",
      "EPOCH 67:\tTraining Loss: 6.525\tValidation loss: 9.909\n",
      "\n",
      "EPOCH 68:\tTraining Loss: 7.064\tValidation loss: 4.480\n",
      "\n",
      "EPOCH 69:\tTraining Loss: 8.238\tValidation loss: 7.917\n",
      "\n",
      "EPOCH 70:\tTraining Loss: 5.275\tValidation loss: 5.055\n",
      "\n",
      "EPOCH 71:\tTraining Loss: 6.831\tValidation loss: 6.576\n",
      "\n",
      "EPOCH 72:\tTraining Loss: 5.437\tValidation loss: 12.544\n",
      "\n",
      "EPOCH 73:\tTraining Loss: 5.024\tValidation loss: 5.066\n",
      "\n",
      "EPOCH 74:\tTraining Loss: 5.685\tValidation loss: 5.587\n",
      "\n",
      "EPOCH 75:\tTraining Loss: 4.813\tValidation loss: 6.277\n",
      "\n",
      "EPOCH 76:\tTraining Loss: 7.981\tValidation loss: 5.713\n",
      "\n",
      "EPOCH 77:\tTraining Loss: 7.767\tValidation loss: 53.919\n",
      "\n",
      "EPOCH 78:\tTraining Loss: 13.046\tValidation loss: 8.948\n",
      "\n",
      "EPOCH 79:\tTraining Loss: 5.521\tValidation loss: 6.468\n",
      "\n",
      "EPOCH 80:\tTraining Loss: 6.801\tValidation loss: 7.002\n",
      "\n",
      "EPOCH 81:\tTraining Loss: 6.543\tValidation loss: 6.961\n",
      "\n",
      "EPOCH 82:\tTraining Loss: 8.508\tValidation loss: 5.252\n",
      "\n",
      "EPOCH 83:\tTraining Loss: 5.025\tValidation loss: 6.050\n",
      "\n",
      "EPOCH 84:\tTraining Loss: 4.219\tValidation loss: 6.060\n",
      "\n",
      "EPOCH 85:\tTraining Loss: 4.981\tValidation loss: 4.939\n",
      "\n",
      "EPOCH 86:\tTraining Loss: 4.780\tValidation loss: 5.373\n",
      "\n",
      "EPOCH 87:\tTraining Loss: 6.480\tValidation loss: 6.298\n",
      "\n",
      "EPOCH 88:\tTraining Loss: 5.946\tValidation loss: 5.834\n",
      "\n",
      "EPOCH 89:\tTraining Loss: 6.071\tValidation loss: 6.371\n",
      "\n",
      "EPOCH 90:\tTraining Loss: 5.951\tValidation loss: 6.006\n",
      "\n",
      "EPOCH 91:\tTraining Loss: 6.745\tValidation loss: 4.848\n",
      "\n",
      "EPOCH 92:\tTraining Loss: 6.489\tValidation loss: 6.372\n",
      "\n",
      "EPOCH 93:\tTraining Loss: 4.702\tValidation loss: 4.953\n",
      "\n",
      "EPOCH 94:\tTraining Loss: 4.626\tValidation loss: 5.858\n",
      "\n",
      "EPOCH 95:\tTraining Loss: 4.971\tValidation loss: 5.245\n",
      "\n",
      "EPOCH 96:\tTraining Loss: 5.952\tValidation loss: 6.085\n",
      "\n",
      "EPOCH 97:\tTraining Loss: 5.083\tValidation loss: 6.282\n",
      "\n",
      "EPOCH 98:\tTraining Loss: 4.545\tValidation loss: 4.958\n",
      "\n",
      "EPOCH 99:\tTraining Loss: 4.916\tValidation loss: 6.187\n",
      "\n",
      "EPOCH 100:\tTraining Loss: 7.463\tValidation loss: 5.117\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "epochs = 100\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_r2 = []\n",
    "val_r2 = []\n",
    "\n",
    "# Training\n",
    "for epoch in range(epochs):\n",
    "    train_loss = 0.0\n",
    "    train_target = []\n",
    "    val_target = []\n",
    "    train_predict = []\n",
    "    val_predict = []\n",
    "    model.train()\n",
    "    for X_samples, y_sample in train_loader:\n",
    "        X_samples = X_samples.to(device)\n",
    "        y_samples = y_sample.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_samples)\n",
    "        train_predict += outputs.tolist()\n",
    "        train_target += outputs.tolist()\n",
    "        loss = criterion(outputs, y_samples)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    train_loss /= len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    train_r2.append(r_squared(train_target, train_predict))\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for X_samples, y_samples in val_loader:\n",
    "            X_samples = X_samples.to(device)\n",
    "            y_samples = y_samples.to(device)\n",
    "            outputs = model(X_samples)\n",
    "            val_predict += outputs.tolist()\n",
    "            val_target += y_samples.tolist()\n",
    "            loss = criterion(outputs, y_samples)\n",
    "            val_loss += loss.item()\n",
    "    val_loss /= len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    val_r2.append(r_squared(val_target, val_predict))\n",
    "    print(f\"\\nEPOCH {epoch + 1}:\\tTraining Loss: {train_loss:.3f}\\tValidation loss: {val_loss:.3f}\")       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on test set: \n",
      "R2: 0.8961698412895203\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_hat = model(X_test.to(device))\n",
    "    test_set_r2 = r_squared(y_hat, y_test)\n",
    "    print(\"Evaluation on test set: \")\n",
    "    print(f\"R2: {test_set_r2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AIO2024-Exercises",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
